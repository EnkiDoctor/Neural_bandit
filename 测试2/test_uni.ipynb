{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math \n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # 使用Xavier初始化来初始化权重\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "\n",
    "        # 初始化偏置为零\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "        \n",
    "# 定义输入、隐藏层和输出的维度\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 10\n",
    "\n",
    "# 创建一个SimpleNN模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customloss(nn.Module):\n",
    "    def __init__(self, theta):\n",
    "        super(customloss, self).__init__()\n",
    "        self.theta = theta\n",
    "    def forward(self, output_list, y_list):\n",
    "        theta = torch.tensor(self.theta, dtype= torch.float32).to(device) \n",
    "        loss = 0\n",
    "        index = 0\n",
    "        for output in output_list:  \n",
    "            y = torch.tensor(y_list[index]).to(device).view(-1,1) \n",
    "            v = torch.mm(output, theta.view(-1,1)) \n",
    "            prob = torch.exp(v) / (torch.sum(torch.exp(v)))  \n",
    "            loss += torch.sum(-y * torch.log(prob)) \n",
    "            index += 1  \n",
    "        return loss / (len(y_list)/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_ARRAY = np.load('features_uni.npy')\n",
    "theta = np.load('theta_gau.npy')\n",
    "PURCHASE_LIST = np.load('purchase_uni.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 82.71349334716797\n",
      "epoch 0, loss 96.50592803955078\n",
      "epoch 0, loss 112.60671997070312\n",
      "epoch 0, loss 111.05846405029297\n",
      "epoch 0, loss 98.6337890625\n",
      "epoch 0, loss 105.79174041748047\n",
      "epoch 0, loss 108.63611602783203\n",
      "epoch 0, loss 104.8917007446289\n",
      "epoch 0, loss 106.26055908203125\n",
      "epoch 0, loss 99.74435424804688\n",
      "epoch 0, loss 97.54106140136719\n",
      "epoch 0, loss 99.97669219970703\n",
      "epoch 0, loss 100.37325286865234\n",
      "epoch 0, loss 97.82245635986328\n",
      "epoch 0, loss 95.50202178955078\n",
      "epoch 0, loss 94.88429260253906\n",
      "epoch 0, loss 93.7434310913086\n",
      "epoch 0, loss 92.20997619628906\n",
      "epoch 0, loss 91.27852630615234\n",
      "epoch 0, loss 91.02237701416016\n",
      "epoch 0, loss 90.10570526123047\n",
      "epoch 0, loss 90.29621124267578\n",
      "epoch 0, loss 91.49471282958984\n",
      "epoch 0, loss 91.43254089355469\n",
      "epoch 0, loss 91.50299072265625\n",
      "epoch 0, loss 91.1216812133789\n",
      "epoch 0, loss 90.50312042236328\n",
      "epoch 0, loss 92.89944458007812\n",
      "epoch 0, loss 91.93199920654297\n",
      "epoch 0, loss 90.56450653076172\n",
      "epoch 0, loss 89.53015899658203\n",
      "epoch 0, loss 90.34737396240234\n",
      "epoch 0, loss 89.7041244506836\n",
      "epoch 0, loss 90.21430206298828\n",
      "epoch 0, loss 89.49015045166016\n",
      "epoch 0, loss 89.5374984741211\n",
      "epoch 0, loss 89.72798156738281\n",
      "epoch 0, loss 90.1863021850586\n",
      "epoch 0, loss 90.35740661621094\n",
      "epoch 0, loss 90.75089263916016\n",
      "epoch 0, loss 90.60778045654297\n",
      "epoch 0, loss 90.88034057617188\n",
      "epoch 0, loss 91.8267593383789\n",
      "epoch 0, loss 91.6649169921875\n",
      "epoch 0, loss 91.47393798828125\n",
      "epoch 0, loss 91.08141326904297\n",
      "epoch 0, loss 90.3724136352539\n",
      "epoch 0, loss 90.79197692871094\n",
      "epoch 0, loss 90.21931457519531\n",
      "epoch 0, loss 89.58993530273438\n",
      "epoch 0, loss 89.50778198242188\n",
      "epoch 0, loss 90.2875747680664\n",
      "epoch 0, loss 90.36803436279297\n",
      "epoch 0, loss 90.79287719726562\n",
      "epoch 0, loss 90.30145263671875\n",
      "epoch 0, loss 90.0649185180664\n",
      "epoch 0, loss 90.74307250976562\n",
      "epoch 0, loss 90.2069091796875\n",
      "epoch 0, loss 89.71080017089844\n",
      "epoch 0, loss 89.7051010131836\n",
      "epoch 0, loss 90.00033569335938\n",
      "epoch 0, loss 89.3959732055664\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\gitproject\\Neural_bandit\\测试2\\test_uni.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/%E6%B5%8B%E8%AF%952/test_uni.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/%E6%B5%8B%E8%AF%952/test_uni.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     output_list \u001b[39m=\u001b[39m [model(torch\u001b[39m.\u001b[39mtensor(a,dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m feature_list]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/%E6%B5%8B%E8%AF%952/test_uni.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(output_list, purchase_list)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/%E6%B5%8B%E8%AF%952/test_uni.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/%E6%B5%8B%E8%AF%952/test_uni.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\bandit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\bandit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32me:\\gitproject\\Neural_bandit\\测试2\\test_uni.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/%E6%B5%8B%E8%AF%952/test_uni.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/%E6%B5%8B%E8%AF%952/test_uni.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m output_list:  \n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/%E6%B5%8B%E8%AF%952/test_uni.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(y_list[index])\u001b[39m.\u001b[39;49mto(device)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m) \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/%E6%B5%8B%E8%AF%952/test_uni.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(output, theta\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)) \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/%E6%B5%8B%E8%AF%952/test_uni.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     prob \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(v) \u001b[39m/\u001b[39m (torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mexp(v)))  \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_list = []\n",
    "purchase_list = []\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "for t in range(0,10000):\n",
    "    context = CONTEXT_ARRAY[t]\n",
    "\n",
    "    purchase_list.append(PURCHASE_LIST[t])\n",
    "\n",
    "    feature_list.append(np.array(context))\n",
    "\n",
    "    if t%10 == 1:\n",
    "        loss_function = customloss(theta)\n",
    "        epochs = 1\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            output_list = [model(torch.tensor(a,dtype = torch.float32).to(device)) for a in feature_list]\n",
    "            loss = loss_function(output_list, purchase_list)\n",
    "     \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "         \n",
    "            print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(86.7751, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treu = [torch.tensor(a,dtype = torch.float32).to(device) for a in feature_list]\n",
    "loss_function(treu,purchase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.85832764e-04 1.61504318e-03 1.71842395e-02 3.13120681e-01\n",
      " 3.59824194e-04 4.86140128e-01 1.81242808e-01 5.14427929e-05]\n",
      "[[1.59856226e-04 1.05465866e-03 1.37029243e-02 3.13274027e-01\n",
      "  2.10107517e-04 4.97273834e-01 1.74299437e-01 2.51556344e-05]]\n"
     ]
    }
   ],
   "source": [
    "problist  = np.load('prob_list.npy')\n",
    "def hhh(i):\n",
    "    ff = model(torch.tensor(CONTEXT_ARRAY[i],dtype = torch.float32).to(device))\n",
    "    ff = ff.cpu().detach().numpy()\n",
    "    u = (ff @ theta.T ).reshape(1,-1)\n",
    "    prob = np.exp(u) / (np.sum(np.exp(u)))\n",
    "    \n",
    "    print(problist[i])\n",
    "    print(prob)\n",
    "\n",
    "hhh(1000)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
