{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math \n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # 使用Xavier初始化来初始化权重\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "\n",
    "        # 初始化偏置为零\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "        \n",
    "# 定义输入、隐藏层和输出的维度\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 10\n",
    "\n",
    "# 创建一个SimpleNN模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customloss(nn.Module):\n",
    "    def __init__(self, theta):\n",
    "        super(customloss, self).__init__()\n",
    "        self.theta = theta\n",
    "    def forward(self, output_list, y_list):\n",
    "        theta = torch.tensor(self.theta, dtype= torch.float32).to(device) \n",
    "        loss = 0\n",
    "        index = 0\n",
    "        for output in output_list:  \n",
    "            y = torch.tensor(y_list[index]).to(device) \n",
    "            v = torch.mm(output, theta.view(-1,1)) \n",
    "            prob = torch.exp(v) / (torch.sum(torch.exp(v)))  \n",
    "            loss += torch.sum(-y * torch.log(prob)) \n",
    "            index += 1  \n",
    "        return loss / (len(y_list)/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_ARRAY = np.load('features_uni.npy')\n",
    "theta = np.load('theta_gau.npy')\n",
    "PURCHASE_LIST = np.load('purchase_uni.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1727.91552734375\n",
      "epoch 0, loss 1694.7509765625\n",
      "epoch 0, loss 1682.581787109375\n",
      "epoch 0, loss 1674.61474609375\n",
      "epoch 0, loss 1671.0302734375\n",
      "epoch 0, loss 1667.1309814453125\n",
      "epoch 0, loss 1664.7103271484375\n",
      "epoch 0, loss 1663.7701416015625\n",
      "epoch 0, loss 1663.67919921875\n",
      "epoch 0, loss 1663.6793212890625\n",
      "epoch 0, loss 1663.73486328125\n",
      "epoch 0, loss 1663.7315673828125\n",
      "epoch 0, loss 1663.6158447265625\n",
      "epoch 0, loss 1663.561767578125\n",
      "epoch 0, loss 1663.5616455078125\n",
      "epoch 0, loss 1663.5625\n",
      "epoch 0, loss 1663.56201171875\n",
      "epoch 0, loss 1663.5615234375\n",
      "epoch 0, loss 1663.561279296875\n",
      "epoch 0, loss 1663.5606689453125\n",
      "epoch 0, loss 1663.559814453125\n",
      "epoch 0, loss 1663.5595703125\n",
      "epoch 0, loss 1663.558349609375\n",
      "epoch 0, loss 1663.5567626953125\n",
      "epoch 0, loss 1663.555908203125\n",
      "epoch 0, loss 1663.5556640625\n",
      "epoch 0, loss 1663.5560302734375\n",
      "epoch 0, loss 1663.5560302734375\n",
      "epoch 0, loss 1663.556396484375\n",
      "epoch 0, loss 1663.556640625\n",
      "epoch 0, loss 1663.5567626953125\n",
      "epoch 0, loss 1663.556640625\n",
      "epoch 0, loss 1663.556640625\n",
      "epoch 0, loss 1663.5572509765625\n",
      "epoch 0, loss 1663.5576171875\n",
      "epoch 0, loss 1663.5579833984375\n",
      "epoch 0, loss 1663.5584716796875\n",
      "epoch 0, loss 1663.5589599609375\n",
      "epoch 0, loss 1663.559326171875\n",
      "epoch 0, loss 1663.5595703125\n",
      "epoch 0, loss 1663.5599365234375\n",
      "epoch 0, loss 1663.5604248046875\n",
      "epoch 0, loss 1663.560791015625\n",
      "epoch 0, loss 1663.5609130859375\n",
      "epoch 0, loss 1663.561279296875\n",
      "epoch 0, loss 1663.5616455078125\n",
      "epoch 0, loss 1663.5618896484375\n",
      "epoch 0, loss 1663.5621337890625\n",
      "epoch 0, loss 1663.562255859375\n",
      "epoch 0, loss 1663.5626220703125\n",
      "epoch 0, loss 1663.562744140625\n",
      "epoch 0, loss 1663.56298828125\n",
      "epoch 0, loss 1663.563232421875\n",
      "epoch 0, loss 1663.5633544921875\n",
      "epoch 0, loss 1663.5635986328125\n",
      "epoch 0, loss 1663.5638427734375\n",
      "epoch 0, loss 1663.56396484375\n",
      "epoch 0, loss 1663.564208984375\n",
      "epoch 0, loss 1663.564208984375\n",
      "epoch 0, loss 1663.5645751953125\n",
      "epoch 0, loss 1663.564697265625\n",
      "epoch 0, loss 1663.5648193359375\n",
      "epoch 0, loss 1663.5650634765625\n",
      "epoch 0, loss 1663.5650634765625\n",
      "epoch 0, loss 1663.565185546875\n",
      "epoch 0, loss 1663.5654296875\n",
      "epoch 0, loss 1663.5655517578125\n",
      "epoch 0, loss 1663.565673828125\n",
      "epoch 0, loss 1663.565673828125\n",
      "epoch 0, loss 1663.56591796875\n",
      "epoch 0, loss 1663.56591796875\n",
      "epoch 0, loss 1663.5660400390625\n",
      "epoch 0, loss 1663.5662841796875\n",
      "epoch 0, loss 1663.5662841796875\n",
      "epoch 0, loss 1663.56640625\n",
      "epoch 0, loss 1663.5665283203125\n",
      "epoch 0, loss 1663.5667724609375\n",
      "epoch 0, loss 1663.5667724609375\n",
      "epoch 0, loss 1663.5667724609375\n",
      "epoch 0, loss 1663.56689453125\n",
      "epoch 0, loss 1663.5670166015625\n",
      "epoch 0, loss 1663.567138671875\n",
      "epoch 0, loss 1663.567138671875\n",
      "epoch 0, loss 1663.5673828125\n",
      "epoch 0, loss 1663.5673828125\n",
      "epoch 0, loss 1663.5673828125\n",
      "epoch 0, loss 1663.5675048828125\n",
      "epoch 0, loss 1663.567626953125\n",
      "epoch 0, loss 1663.5677490234375\n",
      "epoch 0, loss 1663.5677490234375\n",
      "epoch 0, loss 1663.5677490234375\n",
      "epoch 0, loss 1663.56787109375\n",
      "epoch 0, loss 1663.5679931640625\n",
      "epoch 0, loss 1663.568115234375\n",
      "epoch 0, loss 1663.5679931640625\n",
      "epoch 0, loss 1663.568115234375\n",
      "epoch 0, loss 1663.5682373046875\n",
      "epoch 0, loss 1663.5682373046875\n",
      "epoch 0, loss 1663.5684814453125\n",
      "epoch 0, loss 1663.5677490234375\n"
     ]
    }
   ],
   "source": [
    "feature_list = []\n",
    "purchase_list = []\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "for t in range(0,1000):\n",
    "    context = CONTEXT_ARRAY[t]\n",
    "\n",
    "    purchase_list.append(PURCHASE_LIST[t])\n",
    "\n",
    "    feature_list.append(np.array(context))\n",
    "\n",
    "    if t%10 == 1:\n",
    "        loss_function = customloss(theta)\n",
    "        epochs = 1\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            output_list = [model(torch.tensor(a,dtype = torch.float32).to(device)) for a in feature_list]\n",
    "            loss = loss_function(output_list, purchase_list)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "         \n",
    "            print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.47919351e-01 4.88826777e-04 8.65607633e-03 3.62747532e-04\n",
      " 5.65246786e-01 2.92663029e-03 2.73947878e-01 4.51703708e-04]\n",
      "[[0.12503353 0.12492588 0.12497929 0.12507289 0.12505956 0.12495974\n",
      "  0.12504535 0.12492376]]\n"
     ]
    }
   ],
   "source": [
    "problist  = np.load('prob_list.npy')\n",
    "def hhh(i):\n",
    "    ff = model(torch.tensor(CONTEXT_ARRAY[i],dtype = torch.float32).to(device))\n",
    "    ff = ff.cpu().detach().numpy()\n",
    "    u = (ff @ theta.T ).reshape(1,-1)\n",
    "    prob = np.exp(u) / (np.sum(np.exp(u)))\n",
    "    \n",
    "    print(problist[i])\n",
    "    print(prob)\n",
    "\n",
    "hhh(100)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
