{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import math \n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name, K):\n",
    "    if dataset_name == \"statlog\":\n",
    "        ##statlog\n",
    "        __context_size__ = 8\n",
    "        __arm_size__ = 7\n",
    "        fr = open(\"shuttle.trn\",'r+')\n",
    "        CONTEXT = []\n",
    "        REWARD = []\n",
    "        SYM = {}\n",
    "        aa = 0\n",
    "        K = 0\n",
    "        for line in fr:\n",
    "            K+=1\n",
    "            aaa = line.split(\"\\n\")\n",
    "            aaa = aaa[0].split(\" \")\n",
    "            temp = np.int64(aaa[9])-1\n",
    "            context = np.double(aaa[1:9])\n",
    "            context = context/np.linalg.norm(context)\n",
    "            ttt = context\n",
    "            CONTEXT.append(ttt)\n",
    "            reward = np.zeros(__arm_size__)\n",
    "            reward[temp] = 1\n",
    "            REWARD.append(reward)\n",
    "\n",
    "    elif dataset_name == \"magic\":\n",
    "        ####magic gamma\n",
    "\n",
    "        __context_size__ = 10\n",
    "        __arm_size__ = 2\n",
    "        fr = open(\"letter.data\", 'r+')\n",
    "\n",
    "        K = 0\n",
    "        CONTEXT = []\n",
    "        REWARD = []\n",
    "\n",
    "        fr.close()\n",
    "        fr = open(\"magic04.data\", 'r+')\n",
    "        for line in fr:\n",
    "\n",
    "            context = []\n",
    "            aaa = line.split(\",\")\n",
    "            context = np.double(aaa[0:__context_size__])\n",
    "            context = context / np.linalg.norm(context)\n",
    "            K += 1\n",
    "            CONTEXT.append(np.array(context))\n",
    "            __context_size__ = len(context)\n",
    "            reward = np.zeros(__arm_size__)\n",
    "            if aaa[10] == 'g\\n':\n",
    "                reward[0] = 1\n",
    "            else:\n",
    "                reward[1] = 1\n",
    "            REWARD.append(reward)\n",
    "\n",
    "        fr.close()\n",
    "    elif dataset_name == \"covertype\":\n",
    "        ##For covertypr\n",
    "        __context_size__ = 14\n",
    "        __arm_size__ = 7\n",
    "        fr = open(\"covtype.data\", 'r+')\n",
    "        CONTEXT = []\n",
    "        REWARD = []\n",
    "        SYM = {}\n",
    "        aa = 0\n",
    "        K = 0\n",
    "        for line in fr:\n",
    "            K += 1\n",
    "            aaa = line.split(\",\")\n",
    "            __context_size__ = len(aaa) - 1\n",
    "            temp = int(aaa[__context_size__]) - 1\n",
    "            context = np.double(aaa[0:__context_size__])\n",
    "            context = context / np.linalg.norm(context)\n",
    "            ttt = context\n",
    "            CONTEXT.append(ttt)\n",
    "            reward = np.zeros(__arm_size__)\n",
    "            reward[temp] = 1\n",
    "            REWARD.append(reward)\n",
    "    else:\n",
    "        assert('dataset not avaiable')\n",
    "\n",
    "    TEMP_CONTEXT = []\n",
    "    TEMP_REWARD = []\n",
    "    sli = np.random.permutation(K)\n",
    "    for i in range(K):\n",
    "        TEMP_CONTEXT.append(CONTEXT[sli[i]])\n",
    "        TEMP_REWARD.append(REWARD[sli[i]])\n",
    "    CONTEXT = TEMP_CONTEXT\n",
    "    REWARD = TEMP_REWARD\n",
    "\n",
    "    return __context_size__, __arm_size__, CONTEXT, REWARD, sli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters\n",
    "input_size = 15\n",
    "hidden_size = 50\n",
    "output_size = 15\n",
    "num_layers = 10\n",
    "arm_size = 7\n",
    "beta = 0.1\n",
    "lambd = 1\n",
    "H = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "\n",
    "theta = np.random.randn(output_size, 1) / np.sqrt(output_size)\n",
    "theta = torch.tensor(theta, dtype = torch.float32)\n",
    "\n",
    "LAMBDA = lambd * torch.eye(output_size, dtype=torch.float32)\n",
    "bb = torch.zeros(LAMBDA.size()[0], dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 15], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# create the model \n",
    "import modeldefine\n",
    "model = modeldefine.Model(input_size, hidden_size, output_size, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRANS(c, a, arm_size):\n",
    "    #### transfer an array context + action to new context with dimension 2*(__context__ + __armsize__)\n",
    "    dim = len(c)\n",
    "    action = np.zeros(arm_size)\n",
    "    action[a] = 1\n",
    "    c_final = np.append(c, action)\n",
    "    c_final = torch.from_numpy(c_final).to(device)\n",
    "    c_final = c_final.view((len(c_final), 1))\n",
    "    c_final = c_final.repeat(2, 1)\n",
    "    c_final = torch.tensor(c_final, dtype=torch.float32).view(1,-1)\n",
    "    return c_final\n",
    "\n",
    "def UCB(A, phi):\n",
    "    #### ucb term\n",
    "    phi = phi.view(-1,1)\n",
    "    try:\n",
    "        tmp, LU = torch.linalg.solve(phi,A)\n",
    "    except:\n",
    "        A = A.detach().numpy()\n",
    "        phi2 = phi.detach().numpy()\n",
    "        tmp = torch.Tensor(np.linalg.solve(A, phi2))\n",
    "\n",
    "    return torch.sqrt(torch.matmul(torch.transpose(phi,1,0), tmp))\n",
    "\n",
    "def get_action(model, context_info, random_indicator):\n",
    "    ucb = [] \n",
    "    bphi = []\n",
    "    for i in range(arm_size):\n",
    "        temp = TRANS(context_info, i, arm_size).to(device)\n",
    "        bphi.append(temp.cpu())\n",
    "        feature = model(temp).cpu()\n",
    "\n",
    "        first_item = torch.mm(feature, theta)\n",
    "        ucb_item = beta * UCB(LAMBDA, feature)\n",
    "        ucb.append(first_item + ucb_item)\n",
    "    \n",
    "    if random_indicator == 0:\n",
    "        return ucb.index(max(ucb)), bphi[ucb.index(max(ucb))] \n",
    "    else:\n",
    "        num =  np.random.randint(0, arm_size)\n",
    "        return num, bphi[num]\n",
    "    \n",
    "def update_lambda_bb(lambd, bb, context_info, action,reward):\n",
    "    temp = TRANS(context_info, action, arm_size).to(device)\n",
    "    feature = model(temp).cpu()\n",
    "    templambda = lambd + torch.mm(feature.t(), feature)\n",
    "    tempbb = bb + reward * feature.t()\n",
    "    return templambda, tempbb\n",
    "\n",
    "def update_theta(lambd, bb):\n",
    "    return torch.linalg.solve(lambd, bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\杜骏晔\\AppData\\Local\\Temp\\ipykernel_18920\\3790411606.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c_final = torch.tensor(c_final, dtype=torch.float32).view(1,-1)\n"
     ]
    }
   ],
   "source": [
    "THETA_action = []\n",
    "CONTEXT_action = []\n",
    "REWARD_action = []\n",
    "context_size, arm_size, CONTEXT_list, REWARD, sli = load_data(\"statlog\", 15000)\n",
    "\n",
    "for t in range(0,100):\n",
    "    context = CONTEXT_list[t]\n",
    "    if t <= 21:\n",
    "        action, c_act = get_action(model, context, 1)\n",
    "    else: \n",
    "        action, c_act = get_action(model, context, 0) \n",
    "\n",
    "    # get the reward\n",
    "    reward = REWARD[t][action] \n",
    "\n",
    "    # construct the training set for neural network\n",
    "    if t % H == 0:\n",
    "        CONTEXT_action = []\n",
    "        REWARD_action = []\n",
    "        CONTEXT_action = c_act\n",
    "        REWARD_action = torch.tensor([reward],device= device, dtype=torch.float32)\n",
    "    else:\n",
    "        CONTEXT_action = torch.cat((CONTEXT_action, c_act), 0)\n",
    "        REWARD_action = torch.cat((REWARD_action, torch.tensor([reward],device= device, dtype=torch.float32)), 0)\n",
    "\n",
    "    # update lambda, bb and theta\n",
    "    LAMBDA, bb = update_lambda_bb(LAMBDA,bb,context, action, reward)\n",
    "    theta = update_theta(LAMBDA, bb)\n",
    "\n",
    "    if t % H == 0:\n",
    "        THETA_action = []\n",
    "        THETA_action = theta.view(1,-1)\n",
    "    else: \n",
    "        THETA_action = torch.cat((THETA_action, theta.view(1,-1)), 0)\n",
    "    \n",
    "    # update the model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6096, 0.6071, 0.6098,  ..., 0.5341, 0.5318, 0.5327],\n",
       "        [0.3355, 0.3134, 0.3158,  ..., 0.2015, 0.2005, 0.2014],\n",
       "        [0.6991, 0.6964, 0.6995,  ..., 0.6919, 0.6880, 0.6894],\n",
       "        ...,\n",
       "        [0.7442, 0.7437, 0.7467,  ..., 0.7426, 0.7381, 0.7396],\n",
       "        [0.7561, 0.7548, 0.7578,  ..., 0.7594, 0.7541, 0.7557],\n",
       "        [0.7296, 0.7275, 0.7306,  ..., 0.7319, 0.7274, 0.7290]],\n",
       "       device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(model(CONTEXT_action.to(device)), THETA_action.t().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7294, 0.3440, 1.2253,  ..., 0.1113, 1.0948, 3.2113],\n",
       "        [0.6865, 0.2541, 1.0781,  ..., 0.2497, 1.1147, 3.0314],\n",
       "        [0.7134, 0.2894, 1.1854,  ..., 0.1431, 1.0804, 3.1730],\n",
       "        ...,\n",
       "        [0.7306, 0.3033, 1.1899,  ..., 0.1364, 1.0617, 3.1882],\n",
       "        [0.7330, 0.2982, 1.1776,  ..., 0.1660, 1.0597, 3.1886],\n",
       "        [0.7246, 0.2967, 1.1761,  ..., 0.1522, 1.0689, 3.1739]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jk = model(CONTEXT_action.to(device))\n",
    "jk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1527,  0.1527,  0.1510,  ..., -0.0243, -0.0291, -0.0284],\n",
       "        [-0.0975, -0.0873, -0.0898,  ..., -0.3149, -0.3140, -0.3144],\n",
       "        [ 0.0526,  0.0732,  0.0740,  ...,  0.1300,  0.1345,  0.1335],\n",
       "        ...,\n",
       "        [ 0.0551,  0.0229,  0.0228,  ...,  0.1395,  0.1244,  0.1263],\n",
       "        [-0.2532, -0.2693, -0.2671,  ..., -0.2717, -0.2651, -0.2649],\n",
       "        [ 0.3220,  0.3323,  0.3330,  ...,  0.3820,  0.3773,  0.3784]],\n",
       "       grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc = THETA_action.t()\n",
    "bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaa = jk @ bc.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6095652 , 0.60713196, 0.6097884 , ..., 0.534112  , 0.53178   ,\n",
       "        0.53269136],\n",
       "       [0.3354706 , 0.31342348, 0.3158491 , ..., 0.20150429, 0.20051885,\n",
       "        0.20135725],\n",
       "       [0.69914514, 0.69643676, 0.6995219 , ..., 0.69186056, 0.687984  ,\n",
       "        0.6894487 ],\n",
       "       ...,\n",
       "       [0.7442129 , 0.74369943, 0.7466913 , ..., 0.7426453 , 0.7381208 ,\n",
       "        0.73962396],\n",
       "       [0.75612366, 0.75478244, 0.75779575, ..., 0.7593875 , 0.7541054 ,\n",
       "        0.75571346],\n",
       "       [0.7296291 , 0.72749746, 0.7305664 , ..., 0.7318702 , 0.72736484,\n",
       "        0.7289618 ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jk.cpu().detach().numpy() @ bc.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6096, 0.3134, 0.6995, 0.4725, 0.2691, 0.4685, 0.4171, 0.5308, 0.3941,\n",
       "        0.4988, 0.2112, 0.2002, 0.4618, 0.3680, 0.2457, 0.4959, 0.4918, 0.7236,\n",
       "        0.4593, 0.2610, 0.5125, 0.5106, 0.7023, 0.6129, 0.7500, 0.7025, 0.7086,\n",
       "        0.6630, 0.6550, 0.7228, 0.7010, 0.6387, 0.7227, 0.7227, 0.6904, 0.7198,\n",
       "        0.7296, 0.7233, 0.6725, 0.6428, 0.7452, 0.7251, 0.6847, 0.7258, 0.6833,\n",
       "        0.6858, 0.7017, 0.6346, 0.7075, 0.7049, 0.7283, 0.6567, 0.7439, 0.7065,\n",
       "        0.7108, 0.7248, 0.7069, 0.7028, 0.6360, 0.6869, 0.7532, 0.6796, 0.7015,\n",
       "        0.6565, 0.7382, 0.7257, 0.6649, 0.7039, 0.7147, 0.7511, 0.7102, 0.7257,\n",
       "        0.7308, 0.7492, 0.7214, 0.7659, 0.7252, 0.6869, 0.7547, 0.6375, 0.7627,\n",
       "        0.7175, 0.7017, 0.7427, 0.7268, 0.7218, 0.6741, 0.7183, 0.7063, 0.6952,\n",
       "        0.6870, 0.6662, 0.7296, 0.6873, 0.7126, 0.7271, 0.7663, 0.7426, 0.7541,\n",
       "        0.7290], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(jk * bc.t().to(device), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REWARD_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
