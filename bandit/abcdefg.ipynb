{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import math \n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=10, out_features=1, bias=True)\n",
      "    (11): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create the model \n",
    "import modeldefine\n",
    "input_size = 5\n",
    "hidden_size = 10 \n",
    "output_size = 1\n",
    "num_layers = 5\n",
    "\n",
    "model = modeldefine.Model(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0059, -0.3591,  0.2124, -0.0511,  0.6059,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000],\n",
      "        [-0.1608,  0.5579, -0.6366, -0.3898, -0.0051, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000],\n",
      "        [-0.1308,  0.5288,  0.1113,  0.2322, -0.1095, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000],\n",
      "        [-0.4892, -0.2016, -0.0771,  0.2091, -0.2249, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000],\n",
      "        [-0.1482,  0.4760,  0.3872, -0.5996,  0.3546, -0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000],\n",
      "        [ 0.2342,  0.2154,  0.1482, -0.3483, -0.2084,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000],\n",
      "        [ 0.1843,  0.1121,  0.1667, -0.2389, -0.3233,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000],\n",
      "        [-0.1486, -0.0247,  0.4259, -0.1226, -0.2484, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000],\n",
      "        [ 0.9412,  0.0572, -0.2189,  0.0379,  0.1230,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.1153, -0.0306,  0.2702, -0.3515, -0.0083, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0059, -0.3591,  0.2124,\n",
      "         -0.0511,  0.6059],\n",
      "        [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.1608,  0.5579, -0.6366,\n",
      "         -0.3898, -0.0051],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.1308,  0.5288,  0.1113,\n",
      "          0.2322, -0.1095],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.4892, -0.2016, -0.0771,\n",
      "          0.2091, -0.2249],\n",
      "        [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.1482,  0.4760,  0.3872,\n",
      "         -0.5996,  0.3546],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.2342,  0.2154,  0.1482,\n",
      "         -0.3483, -0.2084],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.1843,  0.1121,  0.1667,\n",
      "         -0.2389, -0.3233],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.1486, -0.0247,  0.4259,\n",
      "         -0.1226, -0.2484],\n",
      "        [ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.9412,  0.0572, -0.2189,\n",
      "          0.0379,  0.1230],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.1153, -0.0306,  0.2702,\n",
      "         -0.3515, -0.0083]], device='cuda:0', dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "torch.Size([10])\n",
      "Parameter containing:\n",
      "tensor([-0.1909,  0.0970,  0.4389,  0.4162,  0.0233, -0.1967, -0.1160,  0.1456,\n",
      "        -0.2222, -0.1990], device='cuda:0', requires_grad=True)\n",
      "torch.Size([20, 20])\n",
      "Parameter containing:\n",
      "tensor([[-0.1048, -0.3198, -0.4260,  0.0964,  0.1424,  0.1778, -0.0790, -0.9903,\n",
      "         -0.0657, -0.5174, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.4312, -0.0704,  0.5150,  0.3174, -0.1948, -0.7078,  0.2213,  0.6085,\n",
      "          0.1096,  0.6150, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.5584, -0.3515, -0.1229, -0.0748,  0.0211, -0.2634, -0.6347, -0.3002,\n",
      "         -0.3050, -0.0710, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0943, -0.4484,  0.0624,  0.2497,  0.0876, -0.7261,  0.5048, -0.0384,\n",
      "         -0.0788, -0.1695, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.1034, -0.6128, -0.2394, -0.0642,  0.0683, -0.1744, -0.2103,  0.1521,\n",
      "         -0.1858,  0.0912,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [ 0.3853,  0.2526, -0.1590,  0.0478,  0.5022, -0.5206, -0.6069,  0.1982,\n",
      "          0.1805,  0.5177,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1519, -0.3667, -0.3236, -0.3657, -0.0815, -0.2607,  0.4238, -0.1316,\n",
      "         -0.2037,  0.6026,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.5261,  0.1474, -0.5417,  0.0727,  0.2217, -0.2255, -0.0932,  0.1296,\n",
      "         -0.0153, -0.1165, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "        [-0.0360, -0.7490, -0.9990,  0.2942, -0.1097,  0.3087, -0.1466,  0.3385,\n",
      "         -0.5385, -0.4309, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "        [-0.6058, -0.0181, -0.4006, -0.2667,  0.3064, -0.0876, -0.4556, -0.6200,\n",
      "          0.0814,  0.2238, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.1048, -0.3198, -0.4260,  0.0964,  0.1424,  0.1778,\n",
      "         -0.0790, -0.9903, -0.0657, -0.5174],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.4312, -0.0704,  0.5150,  0.3174, -0.1948, -0.7078,\n",
      "          0.2213,  0.6085,  0.1096,  0.6150],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.5584, -0.3515, -0.1229, -0.0748,  0.0211, -0.2634,\n",
      "         -0.6347, -0.3002, -0.3050, -0.0710],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0943, -0.4484,  0.0624,  0.2497,  0.0876, -0.7261,\n",
      "          0.5048, -0.0384, -0.0788, -0.1695],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.1034, -0.6128, -0.2394, -0.0642,  0.0683, -0.1744,\n",
      "         -0.2103,  0.1521, -0.1858,  0.0912],\n",
      "        [ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.3853,  0.2526, -0.1590,  0.0478,  0.5022, -0.5206,\n",
      "         -0.6069,  0.1982,  0.1805,  0.5177],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.1519, -0.3667, -0.3236, -0.3657, -0.0815, -0.2607,\n",
      "          0.4238, -0.1316, -0.2037,  0.6026],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.5261,  0.1474, -0.5417,  0.0727,  0.2217, -0.2255,\n",
      "         -0.0932,  0.1296, -0.0153, -0.1165],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0360, -0.7490, -0.9990,  0.2942, -0.1097,  0.3087,\n",
      "         -0.1466,  0.3385, -0.5385, -0.4309],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.6058, -0.0181, -0.4006, -0.2667,  0.3064, -0.0876,\n",
      "         -0.4556, -0.6200,  0.0814,  0.2238]], device='cuda:0',\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "torch.Size([10])\n",
      "Parameter containing:\n",
      "tensor([ 0.2277,  0.1285, -0.0357,  0.2440, -0.1512,  0.1322, -0.2876,  0.1771,\n",
      "         0.0288, -0.2670], device='cuda:0', requires_grad=True)\n",
      "torch.Size([20, 20])\n",
      "Parameter containing:\n",
      "tensor([[-7.3662e-01, -1.0337e-01, -1.3717e-01, -3.6140e-02, -4.4309e-01,\n",
      "          1.6324e-01,  3.5799e-01, -1.6648e-01, -1.0993e-02,  2.4764e-01,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 4.4031e-01, -9.7673e-02, -6.9567e-01, -1.4080e-01, -1.4481e-01,\n",
      "          5.1249e-01,  7.3564e-02, -5.7654e-01,  1.4050e-01,  2.0881e-01,\n",
      "          0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 2.7409e-01,  2.6591e-01,  6.0144e-01, -3.7459e-01,  2.5767e-01,\n",
      "          9.3827e-02,  2.6335e-01, -2.0663e-01,  2.0352e-01, -2.0761e-01,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "        [-4.8066e-02,  1.6278e-01, -4.9316e-01,  4.3703e-01,  6.8598e-03,\n",
      "          1.3364e-01, -4.0755e-01,  4.3938e-01, -3.0068e-02,  6.5222e-04,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [-3.4574e-01,  5.9998e-01,  3.1075e-01,  7.0540e-01, -2.7435e-01,\n",
      "          3.4237e-01, -4.1053e-01,  1.1685e-01,  2.1868e-01, -1.5566e-01,\n",
      "         -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "        [-1.7374e-02,  3.2416e-01, -6.8182e-01, -5.2817e-01,  3.2246e-01,\n",
      "         -2.2176e-02, -6.3244e-02, -1.6333e-01, -8.1821e-02, -1.1946e-01,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [-6.3736e-01,  6.5977e-02, -2.5238e-01,  3.1830e-01, -4.1789e-01,\n",
      "          6.5537e-01, -1.2028e-01,  1.0794e-01, -3.9516e-01,  2.8728e-01,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 4.5569e-01,  1.1271e-01,  4.2990e-01,  3.2387e-01,  2.6641e-01,\n",
      "         -6.4511e-02, -6.2063e-02,  8.7542e-02,  1.4861e-01, -2.8728e-01,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "        [-1.5174e-01,  4.4715e-01, -1.1243e-01,  9.9671e-01, -5.5583e-03,\n",
      "         -2.0528e-01,  4.0659e-01,  4.1218e-01, -1.6256e-01,  7.4336e-01,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [-3.8963e-02,  1.4601e-01,  6.8199e-01,  4.1518e-01, -8.7757e-02,\n",
      "          4.5195e-02,  1.3679e-01, -6.9769e-02,  1.6799e-01,  3.2050e-01,\n",
      "         -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -7.3662e-01, -1.0337e-01, -1.3717e-01, -3.6140e-02, -4.4309e-01,\n",
      "          1.6324e-01,  3.5799e-01, -1.6648e-01, -1.0993e-02,  2.4764e-01],\n",
      "        [ 0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          4.4031e-01, -9.7673e-02, -6.9567e-01, -1.4080e-01, -1.4481e-01,\n",
      "          5.1249e-01,  7.3564e-02, -5.7654e-01,  1.4050e-01,  2.0881e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "          2.7409e-01,  2.6591e-01,  6.0144e-01, -3.7459e-01,  2.5767e-01,\n",
      "          9.3827e-02,  2.6335e-01, -2.0663e-01,  2.0352e-01, -2.0761e-01],\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -4.8066e-02,  1.6278e-01, -4.9316e-01,  4.3703e-01,  6.8598e-03,\n",
      "          1.3364e-01, -4.0755e-01,  4.3938e-01, -3.0068e-02,  6.5222e-04],\n",
      "        [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -3.4574e-01,  5.9998e-01,  3.1075e-01,  7.0540e-01, -2.7435e-01,\n",
      "          3.4237e-01, -4.1053e-01,  1.1685e-01,  2.1868e-01, -1.5566e-01],\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -1.7374e-02,  3.2416e-01, -6.8182e-01, -5.2817e-01,  3.2246e-01,\n",
      "         -2.2176e-02, -6.3244e-02, -1.6333e-01, -8.1821e-02, -1.1946e-01],\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -6.3736e-01,  6.5977e-02, -2.5238e-01,  3.1830e-01, -4.1789e-01,\n",
      "          6.5537e-01, -1.2028e-01,  1.0794e-01, -3.9516e-01,  2.8728e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "          4.5569e-01,  1.1271e-01,  4.2990e-01,  3.2387e-01,  2.6641e-01,\n",
      "         -6.4511e-02, -6.2063e-02,  8.7542e-02,  1.4861e-01, -2.8728e-01],\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -1.5174e-01,  4.4715e-01, -1.1243e-01,  9.9671e-01, -5.5583e-03,\n",
      "         -2.0528e-01,  4.0659e-01,  4.1218e-01, -1.6256e-01,  7.4336e-01],\n",
      "        [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -3.8963e-02,  1.4601e-01,  6.8199e-01,  4.1518e-01, -8.7757e-02,\n",
      "          4.5195e-02,  1.3679e-01, -6.9769e-02,  1.6799e-01,  3.2050e-01]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
      "torch.Size([10])\n",
      "Parameter containing:\n",
      "tensor([-0.0213,  0.1708,  0.1628,  0.2127,  0.0055, -0.1660,  0.1161, -0.2899,\n",
      "         0.1566, -0.2741], device='cuda:0', requires_grad=True)\n",
      "torch.Size([20, 20])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0614,  0.1165, -0.5116, -0.5399, -0.8190,  0.3340, -0.3462, -0.0476,\n",
      "         -0.2653, -0.1142,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.2438, -0.2852,  0.0620,  0.3314, -0.3134, -0.2095, -0.2967,  0.2810,\n",
      "         -0.1505, -0.2258,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "        [-0.2287,  0.5383, -0.3052,  0.2919,  0.2283, -0.1109,  0.1541,  0.1793,\n",
      "         -0.5904, -0.1984, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.0000, -0.0000],\n",
      "        [-0.1119, -0.0841,  0.1180, -0.8685, -0.5371, -0.0739, -0.2814, -0.0425,\n",
      "         -0.3675,  0.2104, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0596,  0.0128, -0.2439,  0.0223, -0.1453,  0.0671, -0.4121, -0.0630,\n",
      "          0.1951,  0.2935,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "        [-0.7864, -0.0490, -0.3495,  0.2554, -0.8201,  0.1676, -1.2290,  0.2174,\n",
      "         -0.1507,  0.0104, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.2287, -0.0042,  0.5918,  0.3983, -0.8191,  0.2050, -0.1051, -0.0726,\n",
      "          0.0762,  0.6250, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3441, -0.0759,  0.4984,  0.1181,  0.0612,  0.1564,  0.0396,  0.1300,\n",
      "          0.1081, -0.0241,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0954,  0.0580,  0.1848, -0.0905, -0.0341, -0.1341, -0.1825, -0.2170,\n",
      "         -0.3796, -0.1845, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0776, -0.3219,  0.0707,  0.1031, -0.3670, -0.2897, -0.2313, -0.3033,\n",
      "         -0.3607, -0.8401, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0614,  0.1165, -0.5116, -0.5399, -0.8190,  0.3340,\n",
      "         -0.3462, -0.0476, -0.2653, -0.1142],\n",
      "        [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.2438, -0.2852,  0.0620,  0.3314, -0.3134, -0.2095,\n",
      "         -0.2967,  0.2810, -0.1505, -0.2258],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.2287,  0.5383, -0.3052,  0.2919,  0.2283, -0.1109,\n",
      "          0.1541,  0.1793, -0.5904, -0.1984],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000,  0.0000, -0.1119, -0.0841,  0.1180, -0.8685, -0.5371, -0.0739,\n",
      "         -0.2814, -0.0425, -0.3675,  0.2104],\n",
      "        [ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0596,  0.0128, -0.2439,  0.0223, -0.1453,  0.0671,\n",
      "         -0.4121, -0.0630,  0.1951,  0.2935],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.7864, -0.0490, -0.3495,  0.2554, -0.8201,  0.1676,\n",
      "         -1.2290,  0.2174, -0.1507,  0.0104],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.2287, -0.0042,  0.5918,  0.3983, -0.8191,  0.2050,\n",
      "         -0.1051, -0.0726,  0.0762,  0.6250],\n",
      "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.3441, -0.0759,  0.4984,  0.1181,  0.0612,  0.1564,\n",
      "          0.0396,  0.1300,  0.1081, -0.0241],\n",
      "        [-0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0954,  0.0580,  0.1848, -0.0905, -0.0341, -0.1341,\n",
      "         -0.1825, -0.2170, -0.3796, -0.1845],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0776, -0.3219,  0.0707,  0.1031, -0.3670, -0.2897,\n",
      "         -0.2313, -0.3033, -0.3607, -0.8401]], device='cuda:0',\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "torch.Size([10])\n",
      "Parameter containing:\n",
      "tensor([-0.3122, -0.0941, -0.0921,  0.1184, -0.1196, -0.2025,  0.0057, -0.0306,\n",
      "        -0.2372,  0.0350], device='cuda:0', requires_grad=True)\n",
      "torch.Size([20, 20])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3328, -0.4710, -0.0204, -0.3370,  0.1333,  0.1539, -0.0733,  0.4718,\n",
      "         -0.0837, -0.0316,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "        [-0.1775,  0.3015,  0.4338,  0.4845,  0.3279, -0.0505,  0.3234,  0.2289,\n",
      "          0.2793,  0.2679, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2668, -0.3211, -0.2927, -0.2442, -0.1074, -0.3472, -0.2929,  0.2531,\n",
      "          0.0369,  0.3384,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1594, -0.3906, -0.1915,  0.0568,  0.2617,  0.4994,  0.0767, -0.0525,\n",
      "         -0.1680,  0.0178,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [ 0.2922, -0.4575,  0.1095, -0.0506,  0.1494, -0.3369, -0.0177,  0.0801,\n",
      "          0.1586,  0.1580,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3240,  0.2114, -0.2859,  0.0421,  0.3882, -0.0819, -0.2892, -0.4039,\n",
      "          0.0501,  0.2621, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4650,  0.1665, -0.4245,  0.1901,  0.2521,  0.2718, -0.0022, -0.3538,\n",
      "          0.1608,  0.1324,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1143, -0.4268, -0.1669,  0.2834, -0.0128,  0.2824, -0.1207, -0.2267,\n",
      "         -0.0815, -0.4753,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.1779, -0.1502, -0.1877,  0.3691, -0.1813, -0.1030,  0.6734,  0.1723,\n",
      "          0.0690, -0.0099, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0815, -0.0758, -0.3192,  0.1239, -0.6625,  0.1430,  0.1760,  0.1125,\n",
      "         -0.2353,  0.2080, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.3328, -0.4710, -0.0204, -0.3370,  0.1333,  0.1539,\n",
      "         -0.0733,  0.4718, -0.0837, -0.0316],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.1775,  0.3015,  0.4338,  0.4845,  0.3279, -0.0505,\n",
      "          0.3234,  0.2289,  0.2793,  0.2679],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.2668, -0.3211, -0.2927, -0.2442, -0.1074, -0.3472,\n",
      "         -0.2929,  0.2531,  0.0369,  0.3384],\n",
      "        [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.1594, -0.3906, -0.1915,  0.0568,  0.2617,  0.4994,\n",
      "          0.0767, -0.0525, -0.1680,  0.0178],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.2922, -0.4575,  0.1095, -0.0506,  0.1494, -0.3369,\n",
      "         -0.0177,  0.0801,  0.1586,  0.1580],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.3240,  0.2114, -0.2859,  0.0421,  0.3882, -0.0819,\n",
      "         -0.2892, -0.4039,  0.0501,  0.2621],\n",
      "        [ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.4650,  0.1665, -0.4245,  0.1901,  0.2521,  0.2718,\n",
      "         -0.0022, -0.3538,  0.1608,  0.1324],\n",
      "        [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.1143, -0.4268, -0.1669,  0.2834, -0.0128,  0.2824,\n",
      "         -0.1207, -0.2267, -0.0815, -0.4753],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000, -0.1779, -0.1502, -0.1877,  0.3691, -0.1813, -0.1030,\n",
      "          0.6734,  0.1723,  0.0690, -0.0099],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0815, -0.0758, -0.3192,  0.1239, -0.6625,  0.1430,\n",
      "          0.1760,  0.1125, -0.2353,  0.2080]], device='cuda:0',\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "torch.Size([10])\n",
      "Parameter containing:\n",
      "tensor([ 0.0085,  0.3159, -0.2228, -0.2498,  0.1439,  0.0366, -0.0633, -0.1542,\n",
      "        -0.1271,  0.2229], device='cuda:0', requires_grad=True)\n",
      "torch.Size([2, 20])\n",
      "Parameter containing:\n",
      "tensor([[-0.0423, -0.9216,  0.6195,  2.1549,  0.0832, -0.2241, -1.6602, -0.2076,\n",
      "          1.0734,  0.7562, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.0423, -0.9216,  0.6195,  2.1549,  0.0832, -0.2241,\n",
      "         -1.6602, -0.2076,  1.0734,  0.7562]], device='cuda:0',\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "torch.Size([1])\n",
      "Parameter containing:\n",
      "tensor([0.2735], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "\n",
    "# 打印每个参数的形状\n",
    "for param in params:\n",
    "    print(param.shape)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "self and mat2 must have the same dtype, but got Float and Double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\gitproject\\Neural_bandit\\bandit\\abcdefg.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/bandit/abcdefg.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/gitproject/Neural_bandit/bandit/abcdefg.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model(data)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\bandit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\bandit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\gitproject\\Neural_bandit\\bandit\\modeldefine.py:27\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     26\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 27\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n\u001b[0;32m     28\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\anaconda\\envs\\bandit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\bandit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\bandit\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: self and mat2 must have the same dtype, but got Float and Double"
     ]
    }
   ],
   "source": [
    "data = torch.randn(1, ).to(device)\n",
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
