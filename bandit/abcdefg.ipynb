{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import math \n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name, K):\n",
    "    if dataset_name == \"statlog\":\n",
    "        ##statlog\n",
    "        __context_size__ = 8\n",
    "        __arm_size__ = 7\n",
    "        fr = open(\"shuttle.trn\",'r+')\n",
    "        CONTEXT = []\n",
    "        REWARD = []\n",
    "        SYM = {}\n",
    "        aa = 0\n",
    "        K = 0\n",
    "        for line in fr:\n",
    "            K+=1\n",
    "            aaa = line.split(\"\\n\")\n",
    "            aaa = aaa[0].split(\" \")\n",
    "            temp = np.int64(aaa[9])-1\n",
    "            context = np.double(aaa[1:9])\n",
    "            context = context/np.linalg.norm(context)\n",
    "            ttt = context\n",
    "            CONTEXT.append(ttt)\n",
    "            reward = np.zeros(__arm_size__)\n",
    "            reward[temp] = 1\n",
    "            REWARD.append(reward)\n",
    "\n",
    "    elif dataset_name == \"magic\":\n",
    "        ####magic gamma\n",
    "\n",
    "        __context_size__ = 10\n",
    "        __arm_size__ = 2\n",
    "        fr = open(\"letter.data\", 'r+')\n",
    "\n",
    "        K = 0\n",
    "        CONTEXT = []\n",
    "        REWARD = []\n",
    "\n",
    "        fr.close()\n",
    "        fr = open(\"magic04.data\", 'r+')\n",
    "        for line in fr:\n",
    "\n",
    "            context = []\n",
    "            aaa = line.split(\",\")\n",
    "            context = np.double(aaa[0:__context_size__])\n",
    "            context = context / np.linalg.norm(context)\n",
    "            K += 1\n",
    "            CONTEXT.append(np.array(context))\n",
    "            __context_size__ = len(context)\n",
    "            reward = np.zeros(__arm_size__)\n",
    "            if aaa[10] == 'g\\n':\n",
    "                reward[0] = 1\n",
    "            else:\n",
    "                reward[1] = 1\n",
    "            REWARD.append(reward)\n",
    "\n",
    "        fr.close()\n",
    "    elif dataset_name == \"covertype\":\n",
    "        ##For covertypr\n",
    "        __context_size__ = 14\n",
    "        __arm_size__ = 7\n",
    "        fr = open(\"covtype.data\", 'r+')\n",
    "        CONTEXT = []\n",
    "        REWARD = []\n",
    "        SYM = {}\n",
    "        aa = 0\n",
    "        K = 0\n",
    "        for line in fr:\n",
    "            K += 1\n",
    "            aaa = line.split(\",\")\n",
    "            __context_size__ = len(aaa) - 1\n",
    "            temp = int(aaa[__context_size__]) - 1\n",
    "            context = np.double(aaa[0:__context_size__])\n",
    "            context = context / np.linalg.norm(context)\n",
    "            ttt = context\n",
    "            CONTEXT.append(ttt)\n",
    "            reward = np.zeros(__arm_size__)\n",
    "            reward[temp] = 1\n",
    "            REWARD.append(reward)\n",
    "    else:\n",
    "        assert('dataset not avaiable')\n",
    "\n",
    "    TEMP_CONTEXT = []\n",
    "    TEMP_REWARD = []\n",
    "    sli = np.random.permutation(K)\n",
    "    for i in range(K):\n",
    "        TEMP_CONTEXT.append(CONTEXT[sli[i]])\n",
    "        TEMP_REWARD.append(REWARD[sli[i]])\n",
    "    CONTEXT = TEMP_CONTEXT\n",
    "    REWARD = TEMP_REWARD\n",
    "\n",
    "    return __context_size__, __arm_size__, CONTEXT, REWARD, sli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters\n",
    "input_size = 15\n",
    "hidden_size = 50\n",
    "output_size = 15\n",
    "num_layers = 10\n",
    "arm_size = 7\n",
    "beta = 0.1\n",
    "lambd = 1\n",
    "H = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "\n",
    "theta = np.random.randn(output_size, 1) / np.sqrt(output_size)\n",
    "theta = torch.tensor(theta, dtype = torch.float32)\n",
    "\n",
    "LAMBDA = lambd * torch.eye(output_size, dtype=torch.float32)\n",
    "bb = torch.zeros(LAMBDA.size()[0], dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 15], dtype=torch.int32)\n",
      "Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=100, out_features=15, bias=True)\n",
      "    (21): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create the model \n",
    "import modeldefine\n",
    "import torch.nn as nn\n",
    "model = modeldefine.Model(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self, theta):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "        self.theta = theta\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        prediction = torch.sum(output * self.theta, dim=1)  # 按元素乘法并求和\n",
    "        return self.mse_loss(prediction, target)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRANS(c, a, arm_size):\n",
    "    #### transfer an array context + action to new context with dimension 2*(__context__ + __armsize__)\n",
    "    dim = len(c)\n",
    "    action = np.zeros(arm_size)\n",
    "    action[a] = 1\n",
    "    c_final = np.append(c, action)\n",
    "    c_final = torch.from_numpy(c_final).to(device)\n",
    "    c_final = c_final.view((len(c_final), 1))\n",
    "    c_final = c_final.repeat(2, 1)\n",
    "    c_final = torch.tensor(c_final, dtype=torch.float32).view(1,-1)\n",
    "    return c_final\n",
    "\n",
    "def UCB(A, phi):\n",
    "    #### ucb term\n",
    "    phi = phi.view(-1,1)\n",
    "    try:\n",
    "        tmp, LU = torch.linalg.solve(phi,A)\n",
    "    except:\n",
    "        A = A.detach().numpy()\n",
    "        phi2 = phi.detach().numpy()\n",
    "        tmp = torch.Tensor(np.linalg.solve(A, phi2))\n",
    "\n",
    "    return torch.sqrt(torch.matmul(torch.transpose(phi,1,0), tmp))\n",
    "\n",
    "def get_action(model, context_info, random_indicator):\n",
    "    ucb = [] \n",
    "    bphi = []\n",
    "    for i in range(arm_size):\n",
    "        temp = TRANS(context_info, i, arm_size).to(device)\n",
    "        bphi.append(temp.cpu())\n",
    "        feature = model(temp).cpu()\n",
    "\n",
    "        first_item = torch.mm(feature, theta)\n",
    "        ucb_item = beta * UCB(LAMBDA, feature)\n",
    "        ucb.append(first_item + ucb_item)\n",
    "    \n",
    "    if random_indicator == 0:\n",
    "        return ucb.index(max(ucb)), bphi[ucb.index(max(ucb))] \n",
    "    else:\n",
    "        num =  np.random.randint(0, arm_size)\n",
    "        return num, bphi[num]\n",
    "    \n",
    "def update_lambda_bb(lambd, bb, context_info, action,reward):\n",
    "    temp = TRANS(context_info, action, arm_size).to(device)\n",
    "    feature = model(temp).cpu()\n",
    "    templambda = lambd + torch.mm(feature.t(), feature)\n",
    "    tempbb = bb + reward * feature.t()\n",
    "    return templambda, tempbb\n",
    "\n",
    "def update_theta(lambd, bb):\n",
    "    return torch.linalg.solve(lambd, bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\杜骏晔\\AppData\\Local\\Temp\\ipykernel_31252\\3790411606.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c_final = torch.tensor(c_final, dtype=torch.float32).view(1,-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.15593959391117096\n",
      "------\n",
      "Epoch [10/10], Loss: 0.07688181102275848\n",
      "------\n",
      "Epoch [10/10], Loss: 0.08950886875391006\n",
      "------\n",
      "Epoch [10/10], Loss: 0.10682892799377441\n",
      "------\n",
      "Epoch [10/10], Loss: 0.08521535992622375\n",
      "------\n",
      "Epoch [10/10], Loss: 0.046864885836839676\n",
      "------\n",
      "Epoch [10/10], Loss: 0.0510440431535244\n",
      "------\n",
      "Epoch [10/10], Loss: 0.03376225382089615\n",
      "------\n",
      "Epoch [10/10], Loss: 0.04148484766483307\n",
      "------\n",
      "Epoch [10/10], Loss: 0.028880877420306206\n",
      "------\n",
      "Epoch [10/10], Loss: 0.007115131244063377\n",
      "------\n",
      "Epoch [10/10], Loss: 0.020038651302456856\n",
      "------\n",
      "Epoch [10/10], Loss: 0.005968964193016291\n",
      "------\n",
      "Epoch [10/10], Loss: 0.00608148192986846\n",
      "------\n",
      "Epoch [10/10], Loss: 0.08919356763362885\n",
      "------\n",
      "Epoch [10/10], Loss: 0.07808692008256912\n",
      "------\n",
      "Epoch [10/10], Loss: 0.02787322923541069\n",
      "------\n",
      "Epoch [10/10], Loss: 0.03176671266555786\n",
      "------\n",
      "Epoch [10/10], Loss: 0.03287498652935028\n",
      "------\n",
      "Epoch [10/10], Loss: 0.028023412451148033\n",
      "------\n",
      "Epoch [10/10], Loss: 0.054505884647369385\n",
      "------\n",
      "Epoch [10/10], Loss: 0.05959605053067207\n",
      "------\n",
      "Epoch [10/10], Loss: 0.031564828008413315\n",
      "------\n",
      "Epoch [10/10], Loss: 0.022996697574853897\n",
      "------\n",
      "Epoch [10/10], Loss: 0.03019724041223526\n",
      "------\n",
      "Epoch [10/10], Loss: 0.020659612491726875\n",
      "------\n",
      "Epoch [10/10], Loss: 0.05379137024283409\n",
      "------\n",
      "Epoch [10/10], Loss: 0.013794636353850365\n",
      "------\n",
      "Epoch [10/10], Loss: 0.037539057433605194\n",
      "------\n",
      "Epoch [10/10], Loss: 0.0716313049197197\n",
      "------\n",
      "Epoch [10/10], Loss: 0.017301682382822037\n",
      "------\n",
      "Epoch [10/10], Loss: 0.031183704733848572\n",
      "------\n",
      "Epoch [10/10], Loss: 0.013934814371168613\n",
      "------\n",
      "Epoch [10/10], Loss: 0.057349517941474915\n",
      "------\n",
      "Epoch [10/10], Loss: 0.025284353643655777\n",
      "------\n",
      "Epoch [10/10], Loss: 0.022191472351551056\n",
      "------\n",
      "Epoch [10/10], Loss: 0.006575833540409803\n",
      "------\n",
      "Epoch [10/10], Loss: 0.0036651359405368567\n",
      "------\n",
      "Epoch [10/10], Loss: 0.012880467809736729\n",
      "------\n",
      "Epoch [10/10], Loss: 0.06591387838125229\n",
      "------\n",
      "Epoch [10/10], Loss: 0.004773919936269522\n",
      "------\n",
      "Epoch [10/10], Loss: 0.04854857176542282\n",
      "------\n",
      "Epoch [10/10], Loss: 0.022846776992082596\n",
      "------\n",
      "Epoch [10/10], Loss: 0.02444763109087944\n",
      "------\n",
      "Epoch [10/10], Loss: 0.0023131624329835176\n",
      "------\n",
      "Epoch [10/10], Loss: 0.045478932559490204\n",
      "------\n",
      "Epoch [10/10], Loss: 0.05402938649058342\n",
      "------\n",
      "Epoch [10/10], Loss: 0.0022822620812803507\n",
      "------\n",
      "Epoch [10/10], Loss: 0.12850342690944672\n",
      "------\n",
      "Epoch [10/10], Loss: 0.06041201949119568\n",
      "------\n",
      "Epoch [10/10], Loss: 0.023019487038254738\n",
      "------\n",
      "Epoch [10/10], Loss: 0.058853644877672195\n",
      "------\n",
      "Epoch [10/10], Loss: 0.026075109839439392\n",
      "------\n",
      "Epoch [10/10], Loss: 0.07497620582580566\n",
      "------\n",
      "Epoch [10/10], Loss: 0.06120046600699425\n",
      "------\n",
      "Epoch [10/10], Loss: 0.04842466115951538\n",
      "------\n",
      "Epoch [10/10], Loss: 0.06406167894601822\n",
      "------\n",
      "Epoch [10/10], Loss: 0.021815426647663116\n",
      "------\n",
      "Epoch [10/10], Loss: 0.008351973257958889\n",
      "------\n",
      "Epoch [10/10], Loss: 0.05433252081274986\n",
      "------\n",
      "Epoch [10/10], Loss: 0.027902087196707726\n",
      "------\n",
      "Epoch [10/10], Loss: 0.03784141317009926\n",
      "------\n",
      "Epoch [10/10], Loss: 0.013088381849229336\n",
      "------\n",
      "Epoch [10/10], Loss: 0.051983706653118134\n",
      "------\n",
      "Epoch [10/10], Loss: 0.06015253812074661\n",
      "------\n",
      "Epoch [10/10], Loss: 0.036609262228012085\n",
      "------\n",
      "Epoch [10/10], Loss: 0.0547492690384388\n",
      "------\n",
      "Epoch [10/10], Loss: 0.018577834591269493\n",
      "------\n",
      "Epoch [10/10], Loss: 0.04547087103128433\n",
      "------\n",
      "Epoch [10/10], Loss: 0.04331812635064125\n",
      "------\n",
      "Epoch [10/10], Loss: 0.03650122880935669\n",
      "------\n",
      "Epoch [10/10], Loss: 0.0634070634841919\n",
      "------\n",
      "Epoch [10/10], Loss: 0.08865541219711304\n",
      "------\n",
      "Epoch [10/10], Loss: 0.016804935410618782\n",
      "------\n",
      "Epoch [10/10], Loss: 0.03729014843702316\n",
      "------\n",
      "Epoch [10/10], Loss: 0.011851185001432896\n",
      "------\n",
      "Epoch [10/10], Loss: 0.0020605202298611403\n",
      "------\n",
      "Epoch [10/10], Loss: 0.06845502555370331\n",
      "------\n",
      "Epoch [10/10], Loss: 0.03664148598909378\n",
      "------\n",
      "Epoch [10/10], Loss: 0.06149910390377045\n",
      "------\n",
      "Epoch [10/10], Loss: 0.02860358916223049\n",
      "------\n",
      "Epoch [10/10], Loss: 0.0072143226861953735\n",
      "------\n",
      "Epoch [10/10], Loss: 0.05609524995088577\n",
      "------\n",
      "Epoch [10/10], Loss: 0.021151212975382805\n",
      "------\n",
      "Epoch [10/10], Loss: 0.005989105440676212\n",
      "------\n",
      "Epoch [10/10], Loss: 0.017588866874575615\n",
      "------\n",
      "Epoch [10/10], Loss: 0.007329670246690512\n",
      "------\n",
      "Epoch [10/10], Loss: 0.04662277176976204\n",
      "------\n",
      "Epoch [10/10], Loss: 0.017919747158885002\n",
      "------\n",
      "Epoch [10/10], Loss: 0.006997752469033003\n",
      "------\n",
      "Epoch [10/10], Loss: 0.006907530128955841\n",
      "------\n",
      "Epoch [10/10], Loss: 0.0021715769544243813\n",
      "------\n",
      "Epoch [10/10], Loss: 0.012234949506819248\n",
      "------\n",
      "Epoch [10/10], Loss: 0.03397926315665245\n",
      "------\n",
      "Epoch [10/10], Loss: 0.012524285353720188\n",
      "------\n",
      "Epoch [10/10], Loss: 0.016597628593444824\n",
      "------\n",
      "Epoch [10/10], Loss: 0.00031337401014752686\n",
      "------\n",
      "Epoch [10/10], Loss: 0.0002711585839278996\n",
      "------\n",
      "Epoch [10/10], Loss: 0.017390785738825798\n",
      "------\n",
      "Epoch [10/10], Loss: 0.005961991380900145\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "THETA_action = []\n",
    "CONTEXT_action = []\n",
    "REWARD_action = []\n",
    "context_size, arm_size, CONTEXT_list, REWARD, sli = load_data(\"statlog\", 15000)\n",
    "\n",
    "for t in range(0,10000):\n",
    "    context = CONTEXT_list[t]\n",
    "    if t <= 21:\n",
    "        action, c_act = get_action(model, context, 1)\n",
    "    else: \n",
    "        action, c_act = get_action(model, context, 0) \n",
    "\n",
    "    # get the reward\n",
    "    reward = REWARD[t][action] \n",
    "\n",
    "    # construct the training set for neural network\n",
    "    if t % H == 0:\n",
    "        CONTEXT_action = []\n",
    "        REWARD_action = []\n",
    "        CONTEXT_action = c_act\n",
    "        REWARD_action = torch.tensor([reward],device= device, dtype=torch.float32)\n",
    "    else:\n",
    "        CONTEXT_action = torch.cat((CONTEXT_action, c_act), 0)\n",
    "        REWARD_action = torch.cat((REWARD_action, torch.tensor([reward],device= device, dtype=torch.float32)), 0)\n",
    "\n",
    "    # update lambda, bb and theta\n",
    "    LAMBDA, bb = update_lambda_bb(LAMBDA,bb,context, action, reward)\n",
    "    theta = update_theta(LAMBDA, bb)\n",
    "\n",
    "    if t % H == 0:\n",
    "        THETA_action = []\n",
    "        THETA_action = theta.view(1,-1)\n",
    "    else: \n",
    "        THETA_action = torch.cat((THETA_action, theta.view(1,-1)), 0)\n",
    "    \n",
    "    if (t % H == (H -1)):\n",
    "        # update the model\n",
    "        loss_function = CustomMSELoss(THETA_action.detach().to(device))\n",
    "\n",
    "        epochs = 10\n",
    "        for epoch in range(epochs):\n",
    "            # 前向传播\n",
    "            outputs = model(CONTEXT_action.to(device))\n",
    "            loss = loss_function(outputs, REWARD_action)\n",
    "            \n",
    "            # 打印损失\n",
    "            if epoch == 9: print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}\")\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()  # 重要：每次迭代前清零梯度\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'result_folder/1thmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('result_folder/1ththeta.npy', theta.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('result_folder/1thlambda.npy', LAMBDA.detach().numpy())\n",
    "np.save('result_folder/1thbb.npy', bb.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
