{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math \n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        # 使用Xavier初始化来初始化权重\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "\n",
    "        # 初始化偏置为零\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu4(out)\n",
    "        return out\n",
    "        \n",
    "# 定义输入、隐藏层和输出的维度\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 10\n",
    "\n",
    "# 创建一个SimpleNN模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customloss(nn.Module):\n",
    "    def __init__(self, theta):\n",
    "        super(customloss, self).__init__()\n",
    "        self.theta = theta\n",
    "    def forward(self, output_list, y_list):\n",
    "        theta = torch.tensor(self.theta, dtype= torch.float32).to(device) \n",
    "        loss = 0\n",
    "        index = 0\n",
    "        for output in output_list:  \n",
    "            y = torch.tensor(y_list[index]).to(device) \n",
    "            v = torch.mm(output, theta.view(-1,1)) \n",
    "            prob = torch.exp(v) / (torch.sum(torch.exp(v)))  \n",
    "            loss += torch.sum(-y * torch.log(prob) - (1-y) * torch.log(1-prob)) \n",
    "            index += 1  \n",
    "        return loss / (len(y_list)/100)\n",
    "\n",
    "class CustomLoss2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss2, self).__init__()\n",
    "\n",
    "    def forward(self, output_list, y_list, theta):\n",
    "        theta = torch.tensor(theta, dtype=torch.float32).to(device)\n",
    "        loss = 0\n",
    "        for output, y in zip(output_list, y_list):\n",
    "            y = torch.tensor(y).to(device) \n",
    "            v = torch.matmul(output, theta.view(-1,1))\n",
    "            prob = torch.exp(v) / torch.sum(torch.exp(v))\n",
    "            loss += torch.sum(-y * torch.log(prob) - (1-y) * torch.log(1-prob))\n",
    "        return loss / len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_ARRAY = np.load('features1.npy')\n",
    "theta = np.load('theta1.npy')\n",
    "PURCHASE_LIST = np.load('purchase1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 24.13852882385254\n",
      "epoch 0, loss 24.131473541259766\n",
      "epoch 0, loss 24.128826141357422\n",
      "epoch 0, loss 24.128662109375\n",
      "epoch 0, loss 24.127685546875\n",
      "epoch 0, loss 24.127132415771484\n",
      "epoch 0, loss 24.127050399780273\n",
      "epoch 0, loss 24.12675666809082\n",
      "epoch 0, loss 24.126142501831055\n",
      "epoch 0, loss 24.12552261352539\n",
      "epoch 0, loss 24.125221252441406\n",
      "epoch 0, loss 24.12517738342285\n",
      "epoch 0, loss 24.124704360961914\n",
      "epoch 0, loss 24.124469757080078\n",
      "epoch 0, loss 24.124183654785156\n",
      "epoch 0, loss 24.123916625976562\n",
      "epoch 0, loss 24.123685836791992\n",
      "epoch 0, loss 24.12352180480957\n",
      "epoch 0, loss 24.12343978881836\n",
      "epoch 0, loss 24.123416900634766\n",
      "epoch 0, loss 24.1231632232666\n",
      "epoch 0, loss 24.122879028320312\n",
      "epoch 0, loss 24.12278938293457\n",
      "epoch 0, loss 24.122514724731445\n",
      "epoch 0, loss 24.122350692749023\n",
      "epoch 0, loss 24.122177124023438\n",
      "epoch 0, loss 24.121973037719727\n",
      "epoch 0, loss 24.121782302856445\n",
      "epoch 0, loss 24.121583938598633\n",
      "epoch 0, loss 24.12140464782715\n",
      "epoch 0, loss 24.121248245239258\n",
      "epoch 0, loss 24.12112045288086\n",
      "epoch 0, loss 24.12092399597168\n",
      "epoch 0, loss 24.12076759338379\n",
      "epoch 0, loss 24.120628356933594\n",
      "epoch 0, loss 24.1204776763916\n",
      "epoch 0, loss 24.120309829711914\n",
      "epoch 0, loss 24.120161056518555\n",
      "epoch 0, loss 24.120012283325195\n",
      "epoch 0, loss 24.11995506286621\n",
      "epoch 0, loss 24.119863510131836\n",
      "epoch 0, loss 24.119718551635742\n",
      "epoch 0, loss 24.11958885192871\n",
      "epoch 0, loss 24.119489669799805\n",
      "epoch 0, loss 24.119417190551758\n",
      "epoch 0, loss 24.11931037902832\n",
      "epoch 0, loss 24.119163513183594\n",
      "epoch 0, loss 24.11907386779785\n",
      "epoch 0, loss 24.118955612182617\n",
      "epoch 0, loss 24.118865966796875\n",
      "epoch 0, loss 24.1187744140625\n",
      "epoch 0, loss 24.118690490722656\n",
      "epoch 0, loss 24.11857795715332\n",
      "epoch 0, loss 24.118486404418945\n",
      "epoch 0, loss 24.1184139251709\n",
      "epoch 0, loss 24.118350982666016\n",
      "epoch 0, loss 24.118255615234375\n",
      "epoch 0, loss 24.118196487426758\n",
      "epoch 0, loss 24.11812400817871\n",
      "epoch 0, loss 24.118061065673828\n",
      "epoch 0, loss 24.117998123168945\n",
      "epoch 0, loss 24.117923736572266\n",
      "epoch 0, loss 24.117834091186523\n",
      "epoch 0, loss 24.117778778076172\n",
      "epoch 0, loss 24.11772918701172\n",
      "epoch 0, loss 24.117650985717773\n",
      "epoch 0, loss 24.11758041381836\n",
      "epoch 0, loss 24.117549896240234\n",
      "epoch 0, loss 24.117477416992188\n",
      "epoch 0, loss 24.11742401123047\n",
      "epoch 0, loss 24.11735725402832\n",
      "epoch 0, loss 24.117307662963867\n",
      "epoch 0, loss 24.117238998413086\n",
      "epoch 0, loss 24.1171817779541\n",
      "epoch 0, loss 24.117111206054688\n",
      "epoch 0, loss 24.117053985595703\n",
      "epoch 0, loss 24.117000579833984\n",
      "epoch 0, loss 24.116947174072266\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[39m=\u001b[39m loss_function(output_list, purchase_list,theta)\n\u001b[1;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     23\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, loss \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/anaconda3/envs/yangyu/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/yangyu/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_list = []\n",
    "purchase_list = []\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "for t in range(0,10000):\n",
    "    context = CONTEXT_ARRAY[t]\n",
    "\n",
    "    purchase_list.append(PURCHASE_LIST[t])\n",
    "\n",
    "    feature_list.append(np.array(context))\n",
    "\n",
    "    if t%10 == 1:\n",
    "        loss_function = CustomLoss2()\n",
    "        epochs = 1\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            output_list = [model(torch.tensor(a,dtype = torch.float32).to(device)) for a in feature_list]\n",
    "            loss = loss_function(output_list, purchase_list,theta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "         \n",
    "            print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'prob_list.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m problist  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mprob_list.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhhh\u001b[39m(i):\n\u001b[1;32m      3\u001b[0m     ff \u001b[39m=\u001b[39m model(torch\u001b[39m.\u001b[39mtensor(CONTEXT_ARRAY[i],dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/anaconda3/envs/yangyu/lib/python3.9/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'prob_list.npy'"
     ]
    }
   ],
   "source": [
    "problist  = np.load('prob_list.npy')\n",
    "def hhh(i):\n",
    "    ff = model(torch.tensor(CONTEXT_ARRAY[i],dtype = torch.float32).to(device))\n",
    "    ff = ff.cpu().detach().numpy()\n",
    "    u = (ff @ theta.T ).reshape(1,-1)\n",
    "    prob = np.exp(u) / (np.sum(np.exp(u)))\n",
    "    \n",
    "    print(problist[i])\n",
    "    print(prob)\n",
    "\n",
    "hhh(100)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yangyu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "c360bbb524c6422666c75fec3f379d3ab78b6d9fcf0228bd30b04bfdc96da4c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
