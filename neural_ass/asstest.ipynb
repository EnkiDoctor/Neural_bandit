{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math \n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import modeldefine\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "\n",
    "def UCB(A, phi):\n",
    "    #### ucb term\n",
    "    phi = phi.view(-1,1)\n",
    "    try:\n",
    "        tmp, LU = torch.linalg.solve(phi,A)\n",
    "    except:\n",
    "        A = A.detach().numpy()\n",
    "        phi2 = phi.detach().numpy()\n",
    "        tmp = torch.Tensor(np.linalg.solve(A, phi2))\n",
    "\n",
    "    return torch.sqrt(torch.matmul(torch.transpose(phi,1,0), tmp))\n",
    "\n",
    "def calculate_v(contextinfo_list, A, theta):\n",
    "    vj_list = []\n",
    "    feature_list = []\n",
    "    for i in contextinfo_list:\n",
    "        feature = model(i.to(device)).cpu()\n",
    "        first_item =  torch.mm( feature.view(1,-1) , theta)\n",
    "        second_item = alpha * UCB(A, feature)\n",
    "        vj_list.append((first_item + second_item).item())\n",
    "        feature_list.append(feature.detach().numpy())\n",
    "    return np.array(vj_list), feature_list\n",
    "\n",
    "def update_A(A, info_subset):\n",
    "    for i in info_subset:\n",
    "        i = torch.tensor(i, dtype=torch.float32,device=device)\n",
    "        feature = model(i.to(device)).view(1,-1).cpu()\n",
    "        A = A + torch.mm(feature.t(), feature)\n",
    "    return A\n",
    "\n",
    "def prob(vj_list):\n",
    "    sum = np.sum(np.exp(vj_list)) + 1\n",
    "    return [np.exp(vj_list[i]) / sum for i in range(len(vj_list))]  \n",
    "\n",
    "def revenue(vj_list, reward_list):\n",
    "    sum = np.sum(np.exp(vj_list)) + 1\n",
    "    return np.sum(np.multiply(np.exp(vj_list), reward_list) / sum)\n",
    "\n",
    "def assort(contextinfo_list, reward_list, vj_list, feature_list):\n",
    "    length = len(vj_list)\n",
    "    # sort the contextinfo_list and vj with descending order of reward_list\n",
    "    sorted_list = sorted(zip(contextinfo_list, vj_list, reward_list, feature_list), key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    contextinfo_list = [x[0] for x in sorted_list]\n",
    "    vj_list = [x[1] for x in sorted_list]\n",
    "    reward_list = [x[2] for x in sorted_list]\n",
    "    feature_list = [x[3] for x in sorted_list]\n",
    "\n",
    "    # calculate the optimal assortment\n",
    "    optimal_assort = []\n",
    "    optimal_reward = 0\n",
    "    index = 1\n",
    "    for i in range(length):\n",
    "        if revenue(vj_list[:index], reward_list[:index]) >= optimal_reward:\n",
    "            optimal_reward = revenue(vj_list[:index], reward_list[:index])\n",
    "            index += 1\n",
    "        else:\n",
    "            break\n",
    "    return contextinfo_list[:index], feature_list[:index]\n",
    "\n",
    "# this is for the linear purchase model when v = x dot theta\n",
    "def get_linear_purchase(feature_list):\n",
    "    true_Vlist = [(TRUE_THETA @ feature_list[i].reshape(-1,1)).item() for  i in range(len(feature_list))]\n",
    "    prob_list = prob(true_Vlist)\n",
    "\n",
    "    # sample item according to prob_list\n",
    "    if random.uniform(0,1) < 1 - np.sum(prob_list):\n",
    "        return np.array([0 for i in range(len(feature_list))])\n",
    "    else:\n",
    "        returnlist = [0 for i in range(len(feature_list))]\n",
    "        indexchoose = random.choices([i for i in range(len(prob_list))], weights = prob_list)[0]\n",
    "        returnlist[indexchoose] = 1\n",
    "        return np.array(returnlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 1\n",
    "def likelihood(theta, feature_list ,y_list):\n",
    "    # feature's dimension is len * dimension , theta is 1*dimension\n",
    "    v_list = np.matmul(feature_list, theta.T).reshape(-1)\n",
    "    ln_prob = np.log(prob(v_list))\n",
    "    summation = ln_prob * y_list\n",
    "    return -1 * np.sum(summation)\n",
    "\n",
    "def likelihood_derivative(theta, feature_list, y_list):\n",
    "    v_list = np.matmul(feature_list, theta.T).reshape(-1)\n",
    "    prob_list = prob(v_list)\n",
    "    summation = np.matmul(np.array(feature_list).T, (y_list - prob_list))\n",
    "    return -1 * summation\n",
    "\n",
    "def likelihood_array(theta, feature_list_list, y_list_list):\n",
    "    summation =  0.5 * lambd * np.dot(theta, theta)\n",
    "    for i in range(len(feature_list_list)):\n",
    "        summation += likelihood(theta, feature_list_list[i], y_list_list[i])\n",
    "    return summation\n",
    "\n",
    "def likelihood_derivative_array(theta, feature_list_list, y_list_list):\n",
    "    summation = 0.5 * lambd * theta\n",
    "    for i in range(len(feature_list_list)):\n",
    "        summation += likelihood_derivative(theta, feature_list_list[i], y_list_list[i])\n",
    "    return summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLikelihoodLoss(nn.Module): \n",
    "    def __init__(self, theta_list):\n",
    "        super(CustomLikelihoodLoss, self).__init__()\n",
    "        self.theta_list = theta_list\n",
    "\n",
    "    def forward(self, output_list, y_list):\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        for output in output_list:  \n",
    "            y = torch.tensor(y_list[index]).to(device) \n",
    "            theta = torch.tensor(self.theta_list[index], dtype= torch.float32).to(device) \n",
    "            v = torch.mm(output, theta.view(-1,1)) \n",
    "            prob = torch.exp(v) / (torch.sum(torch.exp(v)) + 1)  \n",
    "            loss += torch.sum(torch.log(prob) * y)  \n",
    "            index += 1  \n",
    "        return -loss \n",
    "\n",
    "class CustomLikelihoodLoss2(nn.Module):\n",
    "    def __init__(self, theta_list):\n",
    "        super(CustomLikelihoodLoss2, self).__init__()\n",
    "        self.theta_list = theta_list\n",
    "     \n",
    "    def forward(self, output_list, y_list):\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        for output in output_list:\n",
    "            y = torch.tensor(y_list[index]).to(device)\n",
    "            theta = torch.tensor(self.theta_list[index], dtype= torch.float32).to(device)\n",
    "            v = torch.mm(output, theta.view(-1,1))\n",
    "            prob = torch.exp(v) / (torch.sum(torch.exp(v)) + 1)\n",
    "            # ce loss between prob and y\n",
    "            loss += torch.sum(-y * torch.log(prob) - (1-y) * torch.log(1-prob))\n",
    "            index += 1\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5, 10, 10, 10], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import modeldefine\n",
    "import torch.optim as optim\n",
    "model = modeldefine.Model(5,10,10,2).to(device)\n",
    "# 10 20 20 20 20 20  5\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_ARRAY = np.load('linear_data/features.npy') \n",
    "REWARD_ARRAY = np.load('linear_data/rewards.npy')\n",
    "TRUE_THETA = np.load('linear_data/theta.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 215.64425659179688\n",
      "Epoch [2/10], Loss: 246.12042236328125\n",
      "Epoch [3/10], Loss: 218.12203979492188\n",
      "Epoch [4/10], Loss: 215.7548370361328\n",
      "Epoch [5/10], Loss: 215.7279510498047\n",
      "Epoch [6/10], Loss: 215.62779235839844\n",
      "Epoch [7/10], Loss: 224.44773864746094\n",
      "Epoch [8/10], Loss: 215.74293518066406\n",
      "Epoch [9/10], Loss: 215.95787048339844\n",
      "Epoch [10/10], Loss: 218.1403350830078\n",
      "Epoch [1/10], Loss: 231.6016082763672\n",
      "Epoch [2/10], Loss: 234.17178344726562\n",
      "Epoch [3/10], Loss: 231.18064880371094\n",
      "Epoch [4/10], Loss: 231.44735717773438\n",
      "Epoch [5/10], Loss: 231.87271118164062\n",
      "Epoch [6/10], Loss: 231.8607940673828\n",
      "Epoch [7/10], Loss: 231.4060821533203\n",
      "Epoch [8/10], Loss: 231.19825744628906\n",
      "Epoch [9/10], Loss: 231.54348754882812\n",
      "Epoch [10/10], Loss: 231.58200073242188\n",
      "Epoch [1/10], Loss: 392.54803466796875\n",
      "Epoch [2/10], Loss: 392.09466552734375\n",
      "Epoch [3/10], Loss: 391.8996276855469\n",
      "Epoch [4/10], Loss: 393.39239501953125\n",
      "Epoch [5/10], Loss: 391.95037841796875\n",
      "Epoch [6/10], Loss: 391.9376525878906\n",
      "Epoch [7/10], Loss: 392.0906677246094\n",
      "Epoch [8/10], Loss: 392.2345275878906\n",
      "Epoch [9/10], Loss: 392.3150329589844\n",
      "Epoch [10/10], Loss: 392.26995849609375\n",
      "Epoch [1/10], Loss: 255.88088989257812\n",
      "Epoch [2/10], Loss: 255.55020141601562\n",
      "Epoch [3/10], Loss: 255.4282989501953\n",
      "Epoch [4/10], Loss: 255.48448181152344\n",
      "Epoch [5/10], Loss: 255.4008331298828\n",
      "Epoch [6/10], Loss: 255.3805694580078\n",
      "Epoch [7/10], Loss: 255.37062072753906\n",
      "Epoch [8/10], Loss: 255.3664093017578\n",
      "Epoch [9/10], Loss: 255.37875366210938\n",
      "Epoch [10/10], Loss: 255.36875915527344\n",
      "Epoch [1/10], Loss: 736.572265625\n",
      "Epoch [2/10], Loss: 736.063720703125\n",
      "Epoch [3/10], Loss: 735.9407958984375\n",
      "Epoch [4/10], Loss: 735.8642578125\n",
      "Epoch [5/10], Loss: 735.818115234375\n",
      "Epoch [6/10], Loss: 735.8522338867188\n",
      "Epoch [7/10], Loss: 735.854736328125\n",
      "Epoch [8/10], Loss: 735.8193969726562\n",
      "Epoch [9/10], Loss: 735.8157958984375\n",
      "Epoch [10/10], Loss: 735.8382568359375\n",
      "Epoch [1/10], Loss: 150.3974609375\n",
      "Epoch [2/10], Loss: 150.43258666992188\n",
      "Epoch [3/10], Loss: 150.20596313476562\n",
      "Epoch [4/10], Loss: 150.08218383789062\n",
      "Epoch [5/10], Loss: 150.12355041503906\n",
      "Epoch [6/10], Loss: 150.18817138671875\n",
      "Epoch [7/10], Loss: 150.12936401367188\n",
      "Epoch [8/10], Loss: 150.0863494873047\n",
      "Epoch [9/10], Loss: 150.09738159179688\n",
      "Epoch [10/10], Loss: 150.1230926513672\n",
      "Epoch [1/10], Loss: 782.571533203125\n",
      "Epoch [2/10], Loss: 782.53369140625\n",
      "Epoch [3/10], Loss: 782.5652465820312\n",
      "Epoch [4/10], Loss: 782.5343627929688\n",
      "Epoch [5/10], Loss: 782.5029907226562\n",
      "Epoch [6/10], Loss: 782.5087280273438\n",
      "Epoch [7/10], Loss: 782.5283813476562\n",
      "Epoch [8/10], Loss: 782.5223388671875\n",
      "Epoch [9/10], Loss: 782.5037841796875\n",
      "Epoch [10/10], Loss: 782.4957885742188\n",
      "Epoch [1/10], Loss: 346.3338623046875\n",
      "Epoch [2/10], Loss: 346.2478332519531\n",
      "Epoch [3/10], Loss: 346.2135314941406\n",
      "Epoch [4/10], Loss: 346.2080078125\n",
      "Epoch [5/10], Loss: 346.1806945800781\n",
      "Epoch [6/10], Loss: 346.18475341796875\n",
      "Epoch [7/10], Loss: 346.1862487792969\n",
      "Epoch [8/10], Loss: 346.17236328125\n",
      "Epoch [9/10], Loss: 346.17034912109375\n",
      "Epoch [10/10], Loss: 346.1683654785156\n",
      "Epoch [1/10], Loss: 138.64720153808594\n",
      "Epoch [2/10], Loss: 138.92694091796875\n",
      "Epoch [3/10], Loss: 138.65907287597656\n",
      "Epoch [4/10], Loss: 138.63418579101562\n",
      "Epoch [5/10], Loss: 138.64971923828125\n",
      "Epoch [6/10], Loss: 138.64752197265625\n",
      "Epoch [7/10], Loss: 138.6350555419922\n",
      "Epoch [8/10], Loss: 138.6293487548828\n",
      "Epoch [9/10], Loss: 138.63668823242188\n",
      "Epoch [10/10], Loss: 138.6434326171875\n",
      "Epoch [1/10], Loss: 218.3409881591797\n",
      "Epoch [2/10], Loss: 218.2576446533203\n",
      "Epoch [3/10], Loss: 218.14752197265625\n",
      "Epoch [4/10], Loss: 218.11349487304688\n",
      "Epoch [5/10], Loss: 218.15310668945312\n",
      "Epoch [6/10], Loss: 218.1384735107422\n",
      "Epoch [7/10], Loss: 218.12596130371094\n",
      "Epoch [8/10], Loss: 218.11474609375\n",
      "Epoch [9/10], Loss: 218.1123504638672\n",
      "Epoch [10/10], Loss: 218.11221313476562\n"
     ]
    }
   ],
   "source": [
    "data_length = len(CONTEXT_ARRAY)\n",
    "\n",
    "# define the hyperparameters\n",
    "input_size = 20\n",
    "hidden_size = 20\n",
    "output_size = 10\n",
    "num_layers = 10\n",
    "\n",
    "beta = 0.1\n",
    "\n",
    "H = 100\n",
    "\n",
    "# initialize the parameters\n",
    "\n",
    "theta = np.random.randn(output_size) / np.sqrt(output_size)\n",
    "\n",
    "LAMBDA = lambd * torch.eye(output_size, dtype=torch.float32)\n",
    "\n",
    "\n",
    "ass_list = []\n",
    "feature_list = []\n",
    "purchase_list = []\n",
    "theta_list = []\n",
    "for t in range(0,500):\n",
    "    context = CONTEXT_ARRAY[t]\n",
    "    profit = REWARD_ARRAY[t]\n",
    "\n",
    "    theta_tensor = torch.tensor(theta.reshape(-1,1), dtype=torch.float32)\n",
    "    v_array,initial_feature = calculate_v(torch.tensor(context,dtype=torch.float32), LAMBDA, theta_tensor)\n",
    "    assortment, ass_features = assort(context, profit.tolist()[0], v_array.tolist() , initial_feature)\n",
    "    \n",
    "    purchase_vector = get_linear_purchase(assortment)\n",
    "\n",
    "    # add to list\n",
    "    ass_list.append(np.array(assortment))\n",
    "    feature_list.append(np.array(ass_features))\n",
    "    purchase_list.append(purchase_vector)\n",
    "    # update the parameters\n",
    "    LAMBDA = update_A(LAMBDA, assortment)\n",
    "    \n",
    "    # update theta using MLE\n",
    "    \n",
    "    initial_guess = theta\n",
    "    result = minimize(likelihood_array, initial_guess, args=(ass_list, purchase_list), method='SLSQP', \n",
    "                  constraints={'type':'eq', 'fun': likelihood_derivative_array, 'args':(ass_list, purchase_list)})\n",
    "    theta = result.x\n",
    "    #print(\"best parameter\",theta)\n",
    "    theta_list.append(theta)\n",
    "\n",
    "    # update the neural networks\n",
    "\n",
    "    if t % H == 99:\n",
    "        a_list = ass_list[-1*H:]\n",
    "        y_list = purchase_list[-1*H:]\n",
    "\n",
    "        loss_function = CustomLikelihoodLoss2(theta_list)\n",
    "        epochs = 10\n",
    "     \n",
    "        for epoch in range(epochs):\n",
    "            output_list = [model(torch.tensor(a,dtype = torch.float32).to(device)) for a in a_list]\n",
    "            loss = loss_function(output_list, y_list)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        theta_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.65319341, 0.31689818])]\n",
      "[0.4658851687032235, 0.4668972216677474]\n"
     ]
    }
   ],
   "source": [
    "print(prob((ass_list[10] @ TRUE_THETA.reshape(10,1)).reshape(1,-1)))\n",
    "print(prob(calculate_v(torch.tensor(ass_list[10],dtype=torch.float32), LAMBDA, theta_tensor)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.93355882, 1.94060946])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_v(torch.tensor(ass_list[3],dtype=torch.float32), LAMBDA, theta_tensor)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'testmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('testtheta.npy',theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
